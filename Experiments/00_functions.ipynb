{"cells":[{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","#from matplotlib.ticker import PercentFormatter\n","import seaborn as sns\n","from sklearn.metrics import confusion_matrix, roc_auc_score , accuracy_score, precision_score, recall_score,  f1_score, roc_curve, auc\n","\n","# Snowball stemmer was chosen in favor of Porter Stemmer which is a bit more aggressive and tends to remove too much from a word\n","import nltk\n","from nltk.stem.snowball import SnowballStemmer\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","nltk.download(\"punkt\")\n","nltk.download(\"stopwords\")\n","\n","\n"," \n","# unidecode is the library needed for ASCII folding\n","from unidecode import unidecode\n","import string\n","# Compact Language Detector v3 is a very fast and performant algorithm by Google for language detection: more info here: https://pypi.org/project/pycld3/\n","import re\n","import cld3\n","\n","import pickle as pickle\n","\n","from textdistance import damerau_levenshtein, jaro_winkler, sorensen_dice, jaccard, overlap, ratcliff_obershelp,hamming\n","from datetime import datetime\n","from ngram import NGram\n","import warnings\n","import Levenshtein\n","from fuzzywuzzy import fuzz"],"metadata":{"id":"6OG9qmvwj8Mn"},"id":"6OG9qmvwj8Mn","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"d733b6fb-bdfb-4d17-9f96-b1eccd4b502e","metadata":{"id":"d733b6fb-bdfb-4d17-9f96-b1eccd4b502e"},"source":["# Strategy for text treatment"]},{"cell_type":"markdown","id":"0be94569-1c35-440b-985d-87431c051af8","metadata":{"id":"0be94569-1c35-440b-985d-87431c051af8"},"source":["1. Language detection - for now the default language is English and we switcht to French if cld3 detects it. Should we consider other languages too?\n","2. Remove punctuation and special characters\n","3. Tokenization\n","4. Stop-word removal - stop-word removal is language-based and is done before the stemming, otherwise they might not be detected : If Think here, we can throw it because the text here are two small and the language are differents\n","5. Stemming - stemming is performed in favor for lemmatization, as we're going to be working mainly with names and not even entire sentences. Since lemmatizing depends on the sentence context, it would not be a good option here.\n","6. ASCII folding"]},{"cell_type":"code","execution_count":null,"id":"c527da9d-8898-41f8-b9aa-dcba9d01e72e","metadata":{"tags":[],"id":"c527da9d-8898-41f8-b9aa-dcba9d01e72e"},"outputs":[],"source":["name_column_blacklist = [\"feat\", \"and\", \"featuring\", \"et\", \"+\",\"&\", \"vs\"]\n","name_column_regex_replace = {r\"\\'\": \"\", r\"\\s+\": \" \"}"]},{"cell_type":"code","source":["STEMMER_EN = SnowballStemmer(language='english')\n","STEMMER_FR = SnowballStemmer(language='french')"],"metadata":{"id":"OluSThZXUzCx"},"id":"OluSThZXUzCx","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"3fe8c730-506c-4702-b033-d78b20a03019","metadata":{"id":"3fe8c730-506c-4702-b033-d78b20a03019"},"outputs":[],"source":["def make_text_prep_func(row, \n","                        word_blacklist=name_column_blacklist,\n","                        regex_replace=name_column_regex_replace, \n","                        colonne=None) :\n","    \"\"\"\n","      This function treats the input string by going through the following steps:\n","      1. Language detection\n","      2. Remove punctuation and special characters\n","      3. Tekenization\n","      4. Stop-word removal\n","      5. Stemming\n","      6. ASCII folding\n","    \n","      Arguments:\n","      row {str} -- The input string to be treated.\n","      \n","     Returns:\n","      str -- The treated version of the string. \n","    \"\"\"\n","    \n","\n","    if colonne == None :\n","      s = str(row)\n","    else :\n","      s=row[colonne]\n","    \n","    # in the default case use the English stop-words and stemmer\n","    stemmer = STEMMER_EN\n","    stop_words =  word_blacklist\n","\n","    \n","    # convert to lowercase, just to be sure :)\n","    s = s.lower()\n","    \n","    # check if the language is French and switch to the French\n","    s_lang = cld3.get_language(s)\n","    if s_lang[0]==\"fr\":\n","      stemmer = STEMMER_FR\n","      stop_words = word_blacklist\n","\n","\n","    # remove punctuation\n","    s_clean = s.translate(str.maketrans(string.punctuation, ' ' * len(string.punctuation)))\n","\n","    # tokenize the string into words\n","    s_tokens = word_tokenize(s_clean)\n","\n","    # remove the stop-word tokens\n","\n","    s_tokens_no_stop = [word for word in s_tokens if word not in stop_words]\n","    \n","    # join the stemmed tokens together and ASCII fold\n","    s_tokens_stemmed = [stemmer.stem(word) for word in s_tokens_no_stop]\n","    s_ascii = unidecode(\" \".join(s_tokens_stemmed))\n","    \n","    for regex, replace in regex_replace.items():\n","      s_ascii = re.sub(regex, replace, s_ascii)\n","\n","    return(s_ascii.strip())"]},{"cell_type":"code","source":[""],"metadata":{"id":"GmbUZ1bm4o6R"},"id":"GmbUZ1bm4o6R","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"9495f97d-c4ec-4d81-a0dc-16b3166c1965","metadata":{"id":"9495f97d-c4ec-4d81-a0dc-16b3166c1965"},"source":["## Similarity functions"]},{"cell_type":"code","execution_count":null,"id":"d290181f-6fca-4ec5-8cc0-b450b0572715","metadata":{"id":"d290181f-6fca-4ec5-8cc0-b450b0572715"},"outputs":[],"source":["def compound_similarity(row: pd.DataFrame, col1: str, col2: str) :\n","    \n","    \"\"\"This function computes the a compound score measuring the similarity\n","       between two strings. The score is based on the following 7 metrics:\n","         - Damerau-Levenshtein - edit distance that also takes in account transpositions.\n","         - Jaro-Winkler - similarity based on common letters adjusted for the higher likelihood\n","                       spelling to be correct in the beginning of a string.\n","         - n-gram - This similarity is based on the counts of n-grams (sequence of substrings \n","                   of length n) which are matching. It has been emprirically selected that the length\n","                of the n-grams in this case is set to N=2.\n","         - Jaccard - like n-grams without taking into account the cardinality (length) of the\n","            n-grams. Effectively, this gives n-gram similarity score for N=1.\n","         - Sorensen-Dice - Similar logic as Jaccard but with slight adjustments.\n","         - Overlap - measures the 'overlap' between two strings based on the number of common\n","                    characters in them.\n","         - Ratcliff-Obershelp - takes into account the length of the fully matching substrings\n","        but also the number of matching characters from substrings that do not match completely.\n","        \n","    Arguments:\n","      col1 {str} -- The first columns of strings.\n","      col2 {str} -- The second columns of strings.\n","     \n","    Returns:\n","      float -- The mean of the similarity scores coming from the 7 algorithms. 0 means not similar\n","        at all and 1 means that the two strings match perfectly. If Either of the two strings are\n","        empty, the similarity will be treated as 0.\n","    \"\"\"\n","    s1 = row[col1]\n","    s2 = row[col2]\n","    \n","    if s1 is None:\n","        s1 = \"\"\n","    if s2 is None:\n","        s2 = \"\"\n","    if s1 == \"\" and s2 == \"\":\n","        return 0.\n","\n","    scores = [   Levenshtein.ratio(s1, s2),\n","                 jaro_winkler.normalized_similarity(s1, s2),\n","                 jaccard.normalized_similarity(s1, s2),\n","              \n","                 overlap.normalized_similarity(s1, s2),\n","            \n","                 hamming.normalized_similarity(s1, s2),\n","                 fuzz.partial_ratio(s1, s2)/100\n","             ]\n","  \n","    return scores"]},{"cell_type":"markdown","id":"783599b6-a63d-46a0-b5e5-4d00301d96f6","metadata":{"id":"783599b6-a63d-46a0-b5e5-4d00301d96f6"},"source":["# ML functions"]},{"cell_type":"code","execution_count":null,"id":"8aac5f87-38b9-4873-8d06-8f0d7e243be3","metadata":{"id":"8aac5f87-38b9-4873-8d06-8f0d7e243be3"},"outputs":[],"source":["def customize_corr(df: pd.DataFrame) :\n","    \n","    \"\"\" \n","      Customize correlation matrix visually  \n","      \n","    Arguments:\n","        df - dataframe with features\n","    \n","    Returns: \n","    \"\"\"\n","    \n","\n","    plt.figure(figsize=(16, 10))\n","    \n","    # define the mask to set the values in the upper triangle to True\n","    mask = np.triu(np.ones_like(df.corr()))\n","    heatmap = sns.heatmap(df.corr(), mask=mask, vmin=-1, vmax=1, annot=True, cmap='magma')\n","    heatmap.set_title('Lower Correlation Matrix', fontdict={'fontsize':18}, pad=16)"]},{"cell_type":"code","execution_count":null,"id":"19622cf5-e4d0-4a3a-bf92-03b4c76638ae","metadata":{"id":"19622cf5-e4d0-4a3a-bf92-03b4c76638ae"},"outputs":[],"source":["def confusio_matrix(y_test, y_predicted):\n","    \n","       \"\"\"\n","       A  function to visualize  confusion matrix ,\n","       A technique for summarizing the performance of a classification algorithm.\n","   \n","       Arguments:\n","        y_test {binary} -- reference labels\n","        y_predicted {float} -- predicted proba\n","     \n","       Returns:\n","        float -- The mean of the similarity scores coming from the 7 algorithms. 0 means not similar\n","          at all and 1 means that the two strings match perfectly. If Either of the two strings are\n","          empty, the similarity will be tre\n","\n","       \"\"\"\n","    \n","       cm = confusion_matrix(y_test, y_predicted)\n","       tn, fp, fn, tp = confusion_matrix(y_test, y_predicted).astype(int).ravel()\n","       print(\"Accuracy:\", round((tp + tn)/(tp+tn+fp+fn),2))\n","       print(\"Recall:\", round(tp /(tp+fn),2))\n","       print(\"precision:\", round( tp/(tp+fp),2))\n","       plt.figure(figsize=(5,5))\n","       plt.clf()\n","       plt.imshow(cm, interpolation='nearest',cmap=plt.cm.Wistia)\n","       classNames = ['Negative','Positive']\n","       plt.title('Matrice de confusion')\n","       plt.ylabel('True label')\n","       plt.xlabel('Predicted label')\n","       tick_marks = np.arange(len(classNames))\n","       plt.xticks(tick_marks, classNames, rotation=45)\n","       plt.yticks(tick_marks, classNames)\n","       s = [['TN','FP'], ['FN', 'TP']]\n","\n","       for i in range(2):\n","          for j in range(2):\n","              plt.text(j,i, str(s[i][j])+\" = \"+str(cm[i][j]))\n","       plt.show()"]},{"cell_type":"code","execution_count":null,"id":"5dc7a19c-6f99-4227-867d-5b74886de3ee","metadata":{"id":"5dc7a19c-6f99-4227-867d-5b74886de3ee"},"outputs":[],"source":["def  courbe_roc(colonne_ref, colonne_model) :\n","  \n","    \"\"\" \n","      A function to plot ROC curve, present AUC and optimal tresholds for  individual assignment.\n","      By default, in ML algorithm, we assign 1 or 0 based on a threshold of 0.5. \n","      This function return the optimal treshold thatminimise false positive rate and maximise true prositive rate\n","      \n","    Arguments:\n","    \n","        colonne_ref   - column with labels (ref)\n","        colonne_model - probability of the model \n","    \n","    Returns:\n","    \"\"\"\n","    \n","    fpr, tpr, thresholds = roc_curve(colonne_ref,colonne_model)\n","    roc_auc = auc(fpr, tpr)\n","    optimal_thr = thresholds[np.argmin((0-fpr)**2 + (1-tpr)**2)]\n","    optimal_tpr = tpr[thresholds==optimal_thr][0]\n","    optimal_fpr = fpr[thresholds==optimal_thr][0]\n","    plt.figure()\n","    lw = 2\n","    plt.plot(fpr, tpr, color='orange',lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n","    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n","    plt.scatter(optimal_fpr, optimal_tpr, color=\"red\", lw=lw, label=f\"Opt. similarity threshold: {optimal_thr}\\nOpt. TPR:{round(optimal_tpr, 2)}, Opt. FPR: {round(optimal_fpr, 2)}\")\n","    plt.xlim([0.0, 1.0])\n","    plt.ylim([0.0, 1.0])\n","    plt.xlabel('False Positive Rate')\n","    plt.ylabel('True Positive Rate')\n","    plt.title('Matching  ROC curve')\n","    plt.legend(loc=\"lower right\")\n","    plt.show()"]},{"cell_type":"code","source":["def create_vectorizer_and_vectorized_db(df_items, analyzer='word',ngram_range=(1,1)):\n","\n","    \"\"\"\n","    La fonction retourne la base de référence vectorisée + le vectorize lui même\n","    \n","    args : dictionnaire avec les paramètres  : analyzer, ngram_range et use_id pour le vectorizer\n","    \n","    \"\"\"       \n","    \n","\n","    #creation du vectorizer et de la base vectorisée\n","    print(\"**** creation du vectorizer\")\n","    if analyzer == 'word' :\n","      vectorizer = TfidfVectorizer(analyzer = 'word', use_idf = False)\n","      X_train = vectorizer.fit_transform([ document for document in df_items['text_CLEAN'] ])\n","      \n","    else :\n","      vectorizer = TfidfVectorizer(analyzer = 'char', ngram_range=ngram_range , use_idf = False)\n","      X_train = vectorizer.fit_transform(df_items['text_CLEAN'].str.replace(' ', ''))\n","    \n","    #on \"exécute\" le vectorizer et en sortie on a une matrice sparse + le vocabulaire associé\n","    print(\"nb features:\", len(vectorizer.get_feature_names()))\n","\n","    print(\"**** sauvegarde des fichiers en pickle pour la prochaine fois\")\n","    suffix_gram = re.sub(r\"[(,)]\", \"\", str(ngram_range)).replace(' ','_')\n","    with open(REP_INTERMED + analyzer + '_' + suffix_gram+ '_Xtrain.pkl', 'wb') as  f1 :\n","      pickle.dump(X_train, f1) \n","\n","    with open(REP_INTERMED + analyzer + '_'+  suffix_gram+ '_vectorizer.pkl', 'wb') as f2 :\n","      pickle.dump(vectorizer, f2) "],"metadata":{"id":"9G_ZaHglGroF"},"id":"9G_ZaHglGroF","execution_count":null,"outputs":[]},{"cell_type":"code","source":["def reload_data_and_vectorizer(analyzer = None, ngram_range=None) :\n","\n","  if analyzer== None and ngram_range==None :\n","    \n","     with open(REP_INTERMED + 'Xtrain.pkl', 'rb') as f:\n","         X_train = pickle.load(f)\n","    \n","     with open(REP_INTERMED + 'vectorizer.pkl', 'rb') as handle:\n","         vectorizer = pickle.load(handle)\n","  else :\n","\n","      suffix_gram = re.sub(r\"[(,)]\", \"\", str(ngram_range)).replace(' ','_')\n","\n","      with open(REP_INTERMED + analyzer + '_' + suffix_gram+ '_Xtrain.pkl', 'rb') as f:\n","         X_train = pickle.load(f)\n","    \n","      with open(REP_INTERMED + analyzer + '_' + suffix_gram+ '_vectorizer.pkl', 'rb') as handle:\n","         vectorizer = pickle.load(handle)        \n","        \n","  return  X_train, vectorizer\n","\n","\n","def text_to_vect(source_query, vectorizer):\n","\n","    #normalisation\n","    query = make_text_prep_func(source_query, name_column_blacklist, name_column_regex_replace)\n","\n","    #vectorisation du text\n","    vectorized_querie = vectorizer.transform([query])\n","   \n","    return  vectorized_querie"],"metadata":{"id":"pwXt2_5iJxul"},"id":"pwXt2_5iJxul","execution_count":null,"outputs":[]},{"cell_type":"code","source":["def knn_scratch(source_query, vectorizer, X_train, baseline, sample, nb_top):\n","\n","    # source_query = 'Bob Marley Is this love'\n","    if  len(baseline) < nb_top :\n","      nb_top = len(baseline)\n","    indice = source_query[0]\n","    #normalisation\n","    query = make_text_prep_func(source_query[1], name_column_blacklist, name_column_regex_replace)\n","    #vectorisation du text\n","    vectorized_querie = vectorizer.transform([query])\n","    # calcul de la similarité entre la base de ref et la base d'entrée (données tf-idf sur trigrammes)\n","    #on obtient un array\n","    mat_sim = cosine_similarity(X_train, vectorized_querie)\n","\n","    # fonction pour recuperer celle ayant les similarites élevées \n","    #  np.argsort(x[ind]) retourne les indices du vecteur après rearranagement par ordre croissant\n","    def top_k(x, k):\n","        ind = np.argpartition(x, -1 * k)[-1 * k:]                  \n","        return ind[np.argsort(x[ind])]                    \n","\n","    topsim = np.apply_along_axis(lambda x: top_k(x, nb_top), 0, mat_sim)\n","    topsim2 = pd.DataFrame( topsim, columns=['index'])\n","\n","    l_sim = []\n","    for i in list(topsim2['index']):\n","        l_sim.append( float(mat_sim[i])*100.0 )\n","    topsim2['similarity'] = l_sim\n","\n","\n"," \n","   #recupération des infos d'origine\n","    df_nearest = pd.merge(baseline, topsim2, on = 'index', how='inner')\n","\n","    df_nearest[\"rank\"] =  df_nearest['similarity'].rank(method ='min',ascending = False).astype(int)\n","\n","    df_nearest['id_query'] =sample.loc[indice,'n_ref']\n","\n","    df_nearest['text_query'] = make_text_prep_func(sample.loc[indice,'text'])\n","\n","    distances = ['levenshtein', 'jaro_winkler', 'jaccard', 'overlap', 'hamming', 'fuzzy_partial']\n","\n","    col_list_values = df_nearest.apply(lambda p:compound_similarity(p, \"text_CLEAN\", \"text_query\"), axis=1)\n","    \n","    df_nearest.drop(['index','text_query', 'text_CLEAN'], axis=1, inplace=True)\n","\n","    df_nearest = pd.concat([\n","                            df_nearest[['id_query', 'master_id', 'artist', 'title', 'similarity', 'rank']], \n","                            pd.DataFrame(np.column_stack(col_list_values).T, columns=distances )], \n","                          axis= 1\n","                          )\n","\n","    df_nearest.sort_values(by='similarity', ascending=False, inplace=True)\n","    new_row = pd.DataFrame(np.array([[\n","                                    sample.loc[indice,  'n_ref'],\n","                                    sample.loc[indice,  'n_ref'], \n","                                    sample.loc[indice, 'artiste'],\n","                                    sample.loc[indice, 'titre'],\n","                                    float(-1),\n","                                    0,\n","                                    1,\n","                                    1,\n","                                    1,\n","                                    1,\n","                                    1,\n","                                    1,\n","                                  ]]\n","                                  ), \n","                        columns= ['id_query', 'master_id', 'artist', 'title', 'similarity', 'rank'] + distances\n","                        )\n","\n","    df_nearest = new_row.append(df_nearest, ignore_index=True)\n","    \n","    return   df_nearest"],"metadata":{"id":"AvX1XPyu958T"},"id":"AvX1XPyu958T","execution_count":null,"outputs":[]},{"cell_type":"code","source":["def customize_output(i, results, df_baseline, sample) :\n","\n","  \"\"\"\n","   A function to make the output more appealing and informative.\n","\n","\n","    Arguments:\n","        i : index for row identification\n","        results : unsupervised knn api results (list of distances and indexes) \n","        df_baseline : discogs data \n","        sample  :  sample of cdandlp datasets, size = 1000\n","   \n","     Return :\n","             pd.DataFrame\n","    \"\"\"\n","  distances, indices = results[i]\n","  # transform brute result to daframe\n","  temp = pd.concat([ pd.DataFrame(distances).T, pd.DataFrame(indices).T ], axis=1 )\n","  temp.columns = ['distance','index']\n","  temp['id_query'] =sample.loc[i,'n_ref']\n","\n","\n","  # enrcih the dataframe\n","  indexes = indices.flatten().tolist()\n","  df_baseline['index'] = df_baseline.index\n","\n","  neighbors_sklearn_api= pd.merge(\n","                                  df_baseline[df_baseline['index'].isin(indexes)], temp,\n","                                  on = 'index',\n","                                  how ='inner'\n","                                  ).\\\n","                          sort_values(by='distance')\n","  # add a column rang\n","  neighbors_sklearn_api[\"rank\"] = neighbors_sklearn_api['distance'].rank(method ='min',ascending=True).astype(int)\n","\n","\n","\n","  neighbors_sklearn_api['text_query'] = make_text_prep_func(sample.loc[i,'text'])\n","  neighbors_sklearn_api.reset_index(drop=True, inplace=True)\n","  colonnes = ['levenshtein', 'jaro_winkler', 'jaccard', 'overlap', 'hamming', 'fuzzy_partial']\n","\n","  col_list_values = neighbors_sklearn_api.apply(lambda p:compound_similarity(p, \"text_CLEAN\", \"text_query\"), axis=1)\n","\n","  neighbors_sklearn_api[colonnes] = pd.DataFrame(np.column_stack(col_list_values).T, columns=colonnes)\n","  neighbors_sklearn_api.drop(['index','text', 'text_CLEAN'],axis=1,inplace=True)\n","\n","\n","  # add a rows for query 's informations\n","  column_order = ['id_query', 'master_id', 'artist', 'title', 'distance','rank']  +  colonnes\n","\n","  neighbors_sklearn_api = neighbors_sklearn_api[column_order]\n","  new_row = pd.DataFrame(np.array([[\n","                                    sample.loc[i,  'n_ref'],\n","                                    sample.loc[i,  'n_ref'], \n","                                    sample.loc[i, 'artiste'],\n","                                    sample.loc[i, 'titre'],\n","                                    float(-1),\n","                                    0,   1,   1, 1,  1,  1,    1,]])\n","                        , columns=column_order\n","                        )\n","  neighbors_sklearn_api = new_row.append(neighbors_sklearn_api, ignore_index=True)\n","\n","\n","  return neighbors_sklearn_api"],"metadata":{"id":"h3VlAOHRKXDy"},"id":"h3VlAOHRKXDy","execution_count":null,"outputs":[]},{"cell_type":"code","source":["def customize_output1(i, results, df_baseline, sample) :\n","\n","  \"\"\"\n","   A function to make the output more appealing and informative.\n","\n","\n","    Arguments:\n","        i : index for row identification\n","        results : unsupervised knn api results (list of distances and indexes) \n","        df_baseline : discogs data \n","        sample  :  sample of cdandlp datasets, size = 1000\n","   \n","     Return :\n","             pd.DataFrame\n","    \"\"\"\n","  distances, indices = results[i]\n","  # transform brute result to daframe\n","  temp = pd.concat([ pd.DataFrame(distances).T, pd.DataFrame(indices).T ], axis=1 )\n","  temp.columns = ['distance','index']\n","  temp['id_query'] =sample.iloc[i,0]\n","\n","\n","  # enrcih the dataframe\n","  indexes = indices.flatten().tolist()\n","  df_baseline['index'] = df_baseline.index\n","\n","  neighbors_sklearn_api= pd.merge(\n","                                  df_baseline[df_baseline['index'].isin(indexes)], temp,\n","                                  on = 'index',\n","                                  how ='inner'\n","                                  ).\\\n","                          sort_values(by='distance')\n","  # add a column rang\n","  neighbors_sklearn_api[\"rank\"] = neighbors_sklearn_api['distance'].rank(method ='min',ascending=False).astype(int)\n","  neighbors_sklearn_api[\"QUERY\"] = sample.loc[i, 'artiste']\t + \" \"  + sample.loc[i, 'titre']\n","  neighbors_sklearn_api.drop(['index'],axis=1,inplace=True)\n","\n","\n","  return neighbors_sklearn_api"],"metadata":{"id":"z-5SWlohT69r"},"id":"z-5SWlohT69r","execution_count":null,"outputs":[]},{"cell_type":"code","source":["def create_vectorizer(i, df_items, df_sample):\n","\n","    \"\"\"\n","    JUST FOR BLOCKING\n","\n","    La fonction retourne la base de référence vectorisée + le vectorize lui même\n","    \n","    args : dictionnaire avec les paramètres  : analyzer, ngram_range et use_id pour le vectorizer\n","    \n","    \"\"\"      \n","\n","    if df_items.empty:\n","      print('DataFrame is empty!') \n","    else :\n","        #creation du vectorizer et de la base vectorisée\n","        \n","      vectorizer = TfidfVectorizer(analyzer = 'char', ngram_range=(2,2) , use_idf = False)\n","      X_train = vectorizer.fit_transform(df_items['text_CLEAN'].str.replace(' ', ''))\n","      \n","      #on \"exécute\" le vectorizer et en sortie on a une matrice sparse + le vocabulaire associé\n","\n","\n","      suffix_gram = 'batch_' + str(i)\n","\n","\n","      path = REP_INTERMED + suffix_gram+  '_results_sim_cosine.pkl'\n","\n","      nbrs = NearestNeighbors(n_neighbors=20, metric=\"cosine\",algorithm='brute', n_jobs=-1).fit(X_train)\n","\n","\n","      result = [ nbrs.kneighbors(text_to_vect(df_sample.loc[i,'text'], vectorizer)) for i in range (len(df_sample)) ]\n","\n","      print(i)\n","\n","      with open(path, 'wb') as f:\n","        \n","        pickle.dump(result, f) \n","\n","\n","\n"],"metadata":{"id":"-OEaySISLZAG"},"id":"-OEaySISLZAG","execution_count":null,"outputs":[]},{"cell_type":"code","source":["def knn_words_distance(indice, baseline, sample, nb_top):\n"," \n","    \n","    baseline['id_query'] =sample.loc[indice,'n_ref']\n","\n","    # compute distance\n","    distances = ['levenshtein', 'jaro_winkler', 'jaccard', 'overlap', 'hamming', 'fuzzy_partial']\n","    col_list_values = baseline.apply(lambda p:compound_similarity(p, \"text_CLEAN\", \"text_query\"), axis=1)\n","    baseline.drop(['index','text_query', 'text_CLEAN'], axis=1, inplace=True)\n","\n","    # merge distance with inital dataset\n","    df_nearest = pd.concat(  [baseline[['id_query', 'master_id', 'artist', 'title']]\n","                            , pd.DataFrame(np.column_stack(col_list_values).T, columns=distances)], \n","                          axis= 1)\n","    df_nearest.sort_values(by='levenshtein', ascending=False, inplace=True)\n","    df_nearest[\"rank\"] =  df_nearest['levenshtein'].rank(method ='min',ascending = False).astype(int)\n","\n","    # first row for query 's identification\n","    new_row = pd.DataFrame(np.array([[\n","                                    sample.loc[indice,  'n_ref'],\n","                                    sample.loc[indice,  'n_ref'], \n","                                    sample.loc[indice, 'artiste'],\n","                                    sample.loc[indice, 'titre'],\n","                                    1,  1,  1,  1,  1,  1,]]), \n","                        columns= ['id_query', 'master_id', 'artist', 'title'] + distances\n","                        )\n","\n","    df_nearest = new_row.append(df_nearest, ignore_index=True)\n","    \n","    return    df_nearest[df_nearest[\"rank\"]< nb_top]"],"metadata":{"id":"X_e4IL3dQgF2"},"id":"X_e4IL3dQgF2","execution_count":null,"outputs":[]},{"cell_type":"code","source":["def knn_country(df_items, df_sample) :\n","  \n","    # entrainement et vectorisation\n","    vectorizer = TfidfVectorizer(analyzer = 'char', ngram_range=(2,2) , use_idf = False)\n","    X_train = vectorizer.fit_transform(df_items['text_CLEAN'].str.replace(' ', ''))\n","\n","    # exécution du knn\n","    df_items['index'] = df_items.index\n","    func = partial(knn_scratch, vectorizer=vectorizer, X_train=X_train, baseline=df_items.copy(), sample =df_sample.copy(), nb_top=20)\n","    answer = list(map(func,      [[i, df_sample.loc[i,'text']] for  i in range(len(df_sample))]))\n","\n","    # export\n","    with open(REP_INTERMED + df_items.loc[0,'country'] + '_results_sim.pkl', 'wb') as f:\n","        pickle.dump(answer, f) \n","    return answer"],"metadata":{"id":"4ayYa_SRQiB9"},"id":"4ayYa_SRQiB9","execution_count":null,"outputs":[]},{"cell_type":"code","source":["def knn_scratch_(source_query, vectorizer, X_train, baseline, sample, nb_top, indices):\n","  \n","    # source_query = 'Bob Marley Is this love'\n","    if  len(baseline) < nb_top :\n","      nb_top = len(baseline)\n","    indice = source_query[0]\n","    #normalisation\n","    query = make_text_prep_func(source_query[1], name_column_blacklist, name_column_regex_replace)\n","    #vectorisation du text\n","    vectorized_querie = vectorizer.transform([query])\n","    # calcul de la similarité entre la base de ref et la base d'entrée (données tf-idf sur trigrammes)\n","    #on obtient un array\n","    mat_sim = cosine_similarity(X_train[indices], vectorized_querie)\n","\n","    # fonction pour recuperer celle ayant les similarites élevées \n","    #  np.argsort(x[ind]) retourne les indices du vecteur après rearranagement par ordre croissant\n","    def top_k(x, k):\n","        ind = np.argpartition(x, -1 * k)[-1 * k:]                  \n","        return ind[np.argsort(x[ind])]                    \n","\n","    topsim = np.apply_along_axis(lambda x: top_k(x, nb_top), 0, mat_sim)\n","    topsim2 = pd.DataFrame( topsim, columns=['index'])\n","\n","    l_sim = []\n","    for i in list(topsim2['index']):\n","        l_sim.append( float(mat_sim[i])*100.0 )\n","    topsim2['similarity'] = l_sim\n","\n","\n"," \n","   #recupération des infos d'origine\n","    df_nearest = pd.merge(baseline, topsim2, on = 'index', how='inner')\n","\n","    df_nearest[\"rank\"] =  df_nearest['similarity'].rank(method ='min',ascending = False).astype(int)\n","    df_nearest['id_query'] =sample.loc[indice,'n_ref']\n","    df_nearest['text_query'] = make_text_prep_func(sample.loc[indice,'text'])\n","    distances = ['levenshtein', 'jaro_winkler', 'jaccard', 'overlap', 'hamming', 'fuzzy_partial']\n","    col_list_values = df_nearest.apply(lambda p:compound_similarity(p, \"text_CLEAN\", \"text_query\"), axis=1)\n","    df_nearest.drop(['index','text_query', 'text_CLEAN'], axis=1, inplace=True)\n","\n","    df_nearest = pd.concat([\n","                            df_nearest[['id_query', 'master_id', 'artist', 'title', 'similarity', 'rank']], \n","                            pd.DataFrame(np.column_stack(col_list_values).T, columns=distances )],  axis= 1 )\n","\n","    df_nearest.sort_values(by='similarity', ascending=False, inplace=True)\n","    new_row = pd.DataFrame(np.array([[\n","                                    sample.loc[indice,  'n_ref'],\n","                                    sample.loc[indice,  'n_ref'], \n","                                    sample.loc[indice, 'artiste'],\n","                                    sample.loc[indice, 'titre'],\n","                                    float(-1),\n","                                    0,  1, 1, 1, 1,  1,1,]]), \n","                        columns= ['id_query', 'master_id', 'artist', 'title', 'similarity', 'rank'] + distances)\n","    df_nearest = new_row.append(df_nearest, ignore_index=True)\n","    return   df_nearest"],"metadata":{"id":"9Cgj3Sij7J1z"},"id":"9Cgj3Sij7J1z","execution_count":null,"outputs":[]},{"cell_type":"code","source":["#def knn_words(df_items, df_sample, indices) :\n","#\n","#    df_items['index'] = df_items.index\n","#    answer = knn_scratch_([0, df_sample.loc[0,'text']] ,indices =indices ,vectorizer=vectorizer, X_train=X_train, baseline=df_items.copy(), sample =df_sample.copy(), nb_top=20)\n","#    return answer\n"],"metadata":{"id":"Jq4fay78RzN-"},"id":"Jq4fay78RzN-","execution_count":null,"outputs":[]},{"cell_type":"code","source":["def level1_knn_words(index, df, baseline, dict_words):\n","  \"\"\"\n","   A function to run knn for with a dataset of queries.\n","\n","\n","    Arguments:\n","        index : row identification\n","        df_baseline : discogs data \n","        df  :  sample of cdandlp datasets, size = 1000\n","        dict_words  : dictionary (value=words, key=indexes of df_baseline)\n","   \n","     Return :\n","             pd.DataFrame\n","  \"\"\"\n","\n","  row = df.loc[index, 'text']\n","  # retrieve the list of indices by word\n","  indices_list = [dict_words.get(key) for key in row.split()]\n","  distances = ['levenshtein', 'jaro_winkler', 'jaccard', 'overlap', 'hamming', 'fuzzy_partial']\n","  while None in indices_list:\n","      indices_list.remove(None)\n","  # convert the list of lists to flat list\n","  indices_list_flat = list(set([item for sublist in indices_list for item in sublist]))\n","  if indices_list_flat == [] :\n","    new_row = pd.DataFrame(np.array([[\n","                                    df.loc[index,  'n_ref'],\n","                                    df.loc[index,  'n_ref'], \n","                                    df.loc[index, 'artiste'],\n","                                    df.loc[index, 'titre'],\n","                                    float(-1), 0,   1,   1, 1,  1,  1,    1,]])\n","                        , columns= ['id_query', 'master_id', 'artist', 'title', 'similarity','rank'] + distances    \n","\n","                        )\n","    return new_row\n","  # else\n","  # match indexes with baseline indexes\n","  df_baseline = baseline.iloc[indices_list_flat,].reset_index(drop=True)\n","  df_sample = df.iloc[[index,]].reset_index(drop='True')\n","  df_baseline['index'] = df_baseline.index\n","  answer = knn_scratch_([0, df_sample.loc[0,'text']] ,indices =indices_list_flat ,vectorizer=vectorizer, X_train=X_train, baseline=df_baseline.copy(), sample =df_sample.copy(), nb_top=50)\n","  return answer\n"],"metadata":{"id":"f2CIYY4chfN_"},"id":"f2CIYY4chfN_","execution_count":null,"outputs":[]},{"cell_type":"code","source":["def customize_output_blocking(i, results, df_baseline, sample) :\n","\n","  \"\"\"\n","   A function to make the output more appealing and informative.\n","\n","\n","    Arguments:\n","        i : index for row identification\n","        results : unsupervised knn api results (list of distances and indexes) \n","        df_baseline : discogs data \n","        sample  :  sample of cdandlp datasets, size = 1000\n","   \n","     Return :\n","             pd.DataFrame\n","    \"\"\"\n","  distances, indices = results\n","  # transform brute result to daframe\n","  temp = pd.concat([ pd.DataFrame(distances).T, pd.DataFrame(indices).T ], axis=1 )\n","  temp.columns = ['distance','index']\n","  temp['id_query'] =sample.loc[i,'n_ref']\n","\n","\n","  # enrcih the dataframe\n","  indexes = indices.flatten().tolist()\n","  df_baseline['index'] = df_baseline.index\n","\n","  neighbors_sklearn_api= pd.merge(\n","                                  df_baseline[df_baseline['index'].isin(indexes)], temp,\n","                                  on = 'index',\n","                                  how ='inner'\n","                                  ).\\\n","                          sort_values(by='distance')\n","  # add a column rang\n","  neighbors_sklearn_api[\"rank\"] = neighbors_sklearn_api['distance'].rank(method ='min',ascending=True).astype(int)\n","\n","\n","\n","  neighbors_sklearn_api['text_query'] = make_text_prep_func(sample.loc[i,'text'])\n","  neighbors_sklearn_api.reset_index(drop=True, inplace=True)\n","  colonnes = ['levenshtein', 'jaro_winkler', 'jaccard', 'overlap', 'hamming', 'fuzzy_partial']\n","\n","  col_list_values = neighbors_sklearn_api.apply(lambda p:compound_similarity(p, \"text_CLEAN\", \"text_query\"), axis=1)\n","\n","  neighbors_sklearn_api[colonnes] = pd.DataFrame(np.column_stack(col_list_values).T, columns=colonnes)\n","\n","  neighbors_sklearn_api.drop(['index', 'text_CLEAN'],axis=1,inplace=True)\n","\n","\n","  # add a rows for query 's informations\n","  column_order = ['id_query', 'master_id', 'artist', 'title', 'distance','rank']  +  colonnes\n","\n","  neighbors_sklearn_api = neighbors_sklearn_api[column_order]\n","  new_row = pd.DataFrame(np.array([[\n","                                    sample.loc[i,  'n_ref'],\n","                                    sample.loc[i,  'n_ref'], \n","                                    sample.loc[i, 'artiste'],\n","                                    sample.loc[i, 'titre'],\n","                                    float(-1),\n","                                    0,   1,   1, 1,  1,  1,    1,]])\n","                        , columns=column_order\n","                        )\n","  neighbors_sklearn_api = new_row.append(neighbors_sklearn_api, ignore_index=True)\n","\n","\n","  return neighbors_sklearn_api"],"metadata":{"id":"KE4yNG0cqr6m"},"id":"KE4yNG0cqr6m","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Recode pressage"],"metadata":{"id":"-aJvWQGuZbiC"},"id":"-aJvWQGuZbiC"},{"cell_type":"code","source":["def recode_pressage(df):\n","        map = {\n","          'Suisse' : 'Switzerland',\n","         'Swiss' : 'Switzerland',\n","         'Union Europeenne' : 'Europe', \n","         'SB' : 'Solomon Islands',\n","         'Fl' : 'Liechtenstein',\n","         'Ukr' : 'Ukraine',\n","         'Ukraineaineaineaineaineaine' : 'Ukraine',\n","         'Ussrsia' : 'Ussr',\n","         'French' : 'France',\n","         'Francais' : 'France',\n","         'Français' : 'France',\n","         'Fr -' : 'France',\n","         'Russia' :'Ussr',\n","         'United Kingdom' : 'Uk',\n","         'United Kingdom' : 'Uk',\n","         'Angleterre' : 'Uk',\n","         'Royaume Uni' : 'Uk',\n","         'Italie': 'Italy',\n","         'Italia': 'Italy',\n","         'Anglais' : 'Uk',\n","         'England': 'Uk',\n","         'Usa': 'Us',\n","         'U.S.A' : 'Us',\n","         'Us.'   : 'Us',\n","         'Belgique' :'Belgium',\n","         'Belge'  :'Belgium',\n","         'España': 'Spain',\n","         'Espagne' : 'Spain',\n","         'Europerope' : 'Europe',\n","         'Grèce' : 'Greece',\n","         'U.K.' : 'Uk',\n","         'Holland' : 'Netherlands',\n","         'Netherlandse' : 'Netherlands',\n","         'Deutschland' : 'Germany',\n","         'Germany.' :\t 'Germany',\n","          '. Germany' :\t 'Germany',\n","          'Japon' : 'Japan',\n","          'E.U' : 'Europe',\n","          'Al'  : 'Albania',\n","          'Eu' : 'Europe',\n","          'London' : 'Uk',\n","           'Gb'  : 'Uk',\n","           'é'  : 'e',\n","           'Ru' : 'Ussr',\n","           'Nederland' : 'Netherlands',\n","           'Netherlandse' : 'Netherlands',\n","           'Brasil' : 'Brazil',\n","           'Bresil' :'Brazil',\n","           'Coreen' : 'South Korea',\n","           'Australiia' : 'Australia',\n","           'Australie' : 'Australia',\n","           'Pays Bas' : 'Netherlands',\n","           'Etats Unis' : 'Us',\n","           'United States' : 'Us',\n","           'Esp' :'Spain',\n","           'G.B' :'Uk',\n","           'Swe' :  'Sweden', \n","           'Original'  : '',\n","           '(Original)' : ''\n","        }\n","\n","\n","        ## standarized  countries' Format \n","        df = df.replace({'pressage' : map},regex=True)\n","        # ++++++++++++++++++ many digits in column pressage ++++++++++++++++++++++++++++\n","        df['pressage'] = df['pressage'].str.replace('\\d+', '')\n","        # ++++++++++++++++++ Needless words ++++++++++++++++++++++++++++\n","        df['pressage'] = df['pressage'].str.replace('(Original)', '')\n","        df['pressage'] = df['pressage'].str.replace('Made In', '')\n","        df['pressage'] = df['pressage'].str.replace('Original', '')\n","        df['pressage'] = df['pressage'].str.replace('Limited', '')\n","        df['pressage'] = df['pressage'].str.replace('Press', '')\n","        df['pressage'] = df['pressage'].str.replace('Biem', '')\n","        df['pressage'] = df['pressage'].str.replace('Biem', '')\n","        # ++++++++++++++++++ Brute recoding++++++++++++++++++++++++++++\n","        df['pressage'] = df['pressage'].str.replace('Europerope', 'Europe')\n","        df['pressage'] = df['pressage'].str.replace('Netherlandse', 'Netherlands')\n","        df['pressage'] = df['pressage'].str.replace('Nl', 'Netherlands')\n","        df['pressage'] = df['pressage'].str.replace('Dutch', 'Netherlands')\n","        df['pressage'] = df['pressage'].str.replace('Swedenden', 'Sweden')\n","        df['pressage'] = df['pressage'].str.replace('Albanialemagn', 'Albania')\n","        df['pressage'] = df['pressage'].str.replace('Albanial', 'Albania')\n","        df['pressage'] = df['pressage'].str.replace('Albaniae', 'Albania')\n","        df['pressage'] = df['pressage'].str.replace('WesGermany', 'Germany')\n","        # ++++++++++++++++++++ NEXT STEP ++++++++++++++++++++++++++++++++++++\n","        df['pressage'] = df['pressage'].str.strip()\n","        df['pressage'] = df['pressage'].str.replace('Usr', 'Ussr')\n","        df['pressage'] = np.where( df['pressage'] == 'German' , 'Germany',  df['pressage'])\n","        df['pressage'] = np.where( df['pressage'] == 'Gdr' , 'Germany',  df['pressage'])\n","        df['pressage'] = np.where( df['pressage'] == 'Bel' , 'Belgium',  df['pressage'])\n","        df['pressage'] = np.where( df['pressage'] == 'European Union' , 'Europe',  df['pressage'])\n","        df['pressage'] = np.where( df['pressage'] == 'Swedenden' , 'Sweden',  df['pressage'])\n","        df['pressage'] = np.where( df['pressage'] == 'Italyn' , 'Italy',  df['pressage'])\n","        df['pressage'] = np.where( df['pressage'] == 'U.K' , 'Uk',  df['pressage'])\n","        df['pressage'] = np.where( df['pressage'] == 'Netherlandsais' , 'Netherlands',  df['pressage'])\n","        df['pressage'] = np.where( df['pressage'] == 'Netherlandsland' , 'Netherlands',  df['pressage'])   \n","        df['pressage'] = np.where( df['pressage'] == 'Albaniamand', 'Albania',df['pressage'])\n","        df['pressage'] = np.where( df['pressage'] == 'Franc -', 'France -',df['pressage'])\n","        df['pressage'] = np.where( df['pressage'] == 'Stereo Germany'\t, 'Germany',df['pressage'])\n","        df['pressage'] = np.where( df['pressage'] =='U.S', 'Us', df['pressage'])\n","        df['pressage'] = np.where( df['pressage'] =='Fra', 'France', df['pressage'])\n","        df['pressage'] = np.where( df['pressage'] == 'Deutchland - Deutchland', ' Germany', df['pressage'])\n","        df['pressage'] = np.where( df['pressage'] == 'Autriche', 'Austria', df['pressage'])\n","        df['pressage'] = np.where( df['pressage'] =='Made In Us', 'Us', df['pressage'])\n","        df['pressage'] = np.where( df['pressage'] == 'Canadien', 'Canada', df['pressage'])\n","        df['pressage'] = np.where( df['pressage'] == 'Can', 'Canada', df['pressage'])\n","        df['pressage'] = np.where( df['pressage'] == 'Pl', 'Poland', df['pressage'])\n","        df['pressage'] = np.where( df['pressage'] == '- Ue', 'Europe', df['pressage'])\n","        df['pressage'] = np.where( df['pressage'] == 'Epc', 'Europe', df['pressage'])\n","        df['pressage'] = np.where( df['pressage'] == 'Ita' , 'Italy', df['pressage'])\n","        # ++++++++++++++++++++ NEXT STEP ++++++++++++++++++++++++++++++++++++\n","        df['pressage'] = df['pressage'].str.strip('.()- ')\n","        df['pressage'] = np.where( df['pressage'] == 'U.S' ,'Us', df['pressage'])\n","        df['pressage'] = np.where( df['pressage'] == 'Hol' , 'Netherlands',  df['pressage'])\n","        df['pressage'] = np.where( df['pressage'] == 'Ho' , 'Netherlands',  df['pressage'])\n","        df['pressage'] = np.where( df['pressage'] == 'Ukraineaine' , 'Ukraine',  df['pressage'])\n","        df['pressage'] = np.where( df['pressage'] == 'Atl Germany' , 'Germany',  df['pressage'])\n","        df['pressage'] = np.where( df['pressage'] == 'Fr' , 'France',  df['pressage'])\n","        df['pressage'] = np.where( df['pressage'] ==  'Ecc', 'Europe', df['pressage'])\n","        df['pressage'] = np.where( df['pressage'] ==  'Eec', 'Europe', df['pressage'])\n","        df['pressage'] = np.where( df['pressage'] == 'Albaniagerie' , 'Albania',  df['pressage'])\n","        df['pressage'] = np.where( df['pressage'] == 'Ue', 'Europe', df['pressage'])\n","        df['pressage'] = np.where( df['pressage'] == 'U.E', 'Europe', df['pressage'])\n","        df['pressage'] = np.where( df['pressage'] == 'Ca', 'Canada', df['pressage'])\n","        df['pressage'] = np.where( df['pressage'] == 'Europeropa', 'Europe', df['pressage'])\n","        df['pressage'] = np.where( df['pressage'] == 'The Netherlands', 'Netherlands', df['pressage'])\n","        df['pressage'] = np.where( df['pressage'] == 'Holl', 'Netherlands', df['pressage'])\n","        df['pressage'] = np.where( df['pressage'] == 'Netherland', 'Netherlands', df['pressage'])\n","        df['pressage'] = np.where( df['pressage'] == 'France C', 'France', df['pressage'])\n","        df['pressage'] = np.where( df['pressage'] == 'German' , 'Germany',  df['pressage'])\n","        df['pressage'] = np.where( df['pressage'] == 'De' , 'Germany',  df['pressage'])\n","        df['pressage'] = np.where( df['pressage'] == 'Ger' , 'Germany',  df['pressage'])\n","        df['pressage'] = np.where( df['pressage'] == 'Atl Germany' , 'Germany',  df['pressage'])\n","        df['pressage'] = np.where( df['pressage'] == 'France Sans' , 'France',  df['pressage'])\n","        df['pressage'] = np.where( df['pressage']\t== 'Türkiye', 'Turkey', df['pressage'])\n","        df['pays']  = df['pressage'].apply(lambda p : list(p.split('-')))\n","        \n","\n","        def recode_country(elements):\n","\n","          for element in elements :\n","              if element.strip() in list_country:\n","                return  element\n","          return 'NaN'\n","\n","        df['pays_clean'] = df['pays'].apply(recode_country)\n","        \n","        return df"],"metadata":{"id":"XQLywtcIQ7C5"},"id":"XQLywtcIQ7C5","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"colab":{"name":"00_functions.ipynb","provenance":[],"collapsed_sections":["-aJvWQGuZbiC"]},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}